---
title: "MUSA 508, Lab 5 - Spatial Machine Learning Pt. 2"
author: "Harris, Fichman, Steif - 2024"
date: 'August, 2024'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot)
library(jtools)     # for regression model plots
library(broom)
library(tufte)
library(rmarkdown)
library(pander)
library(tidycensus)
# functions and data directory
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

palette5 <- c("#25CB10", "#5AB60C", "#8FA108",   "#C48C04", "#FA7800")
```

## Learning objectives

- Build a training and testing environment to validate a regression model

- Examine Errors to understand if they are non-random, in order to diagnose model quality

- Create and examine validation metrics including MAE and MAPE

- Use Moran's I to see if errors are spatially non-random

- Examine generalizability across geographic contexts

## Data Loading

Bring in a wrangled data set - it has some new variables already created.

```{r read_data, results='hide'}
#https://github.com/urbanSpatial/Public-Policy-Analytics-Landing/tree/master/DATA/Chapter3_4


boston.sf <- st_read(file.path(root.dir,"/Chapter3_4/boston_sf_Ch1_wrangled.geojson")) %>% 
  st_set_crs('ESRI:102286')

nhoods <- 
  st_read("https://raw.githubusercontent.com/mafichman/musa_5080_2023/main/Week_4/neighborhoods/bost_nhoods.geojson") %>%
  st_transform('ESRI:102286')

```

## Split Data into Train/Test Set

- What is the size of the split in the data partition?

- What do the errors messages mean?  If there is a single record in a category - it gets sorted into the training set. Why??

- Can you figure out what's going on when we create our error metrics?

```{r}

inTrain <- createDataPartition(
              y = paste(boston.sf$Name, boston.sf$NUM_FLOORS.cat, 
                        boston.sf$Style, boston.sf$R_AC), 
              p = .60, list = FALSE)
boston.training <- boston.sf[inTrain,] 
boston.test <- boston.sf[-inTrain,]  
 
reg.training <- 
  lm(SalePrice ~ ., data = as.data.frame(boston.training) %>% 
                             dplyr::select(SalePrice, LivingArea, Style, 
                                           GROSS_AREA, NUM_FLOORS.cat,
                                           R_BDRMS, R_FULL_BTH, R_HALF_BTH, 
                                           R_KITCH, R_AC, R_FPLACE, crimes.Buffer, Name))

boston.test <-
  boston.test %>%
  mutate(Regression = "Baseline Regression",
         SalePrice.Predict = predict(reg.training, boston.test),
         SalePrice.Error = SalePrice.Predict - SalePrice,
         SalePrice.AbsError = abs(SalePrice.Predict - SalePrice),
         SalePrice.APE = (abs(SalePrice.Predict - SalePrice)) / SalePrice) %>%
  filter(SalePrice < 5000000) 

```

What is the MAE?

```{r}
mean(boston.test$SalePrice.AbsError, na.rm = T)

```

How about the MAPE?

```{r}
mean(boston.test$SalePrice.APE, na.rm = T)

```

## Cross-Validation

To set up an iteration of random 'folds' (in this case 100), we use our very same regression formula to train a different regression object `reg.cv` that is subjected to the cross-validation process for the whole data set.

If you get warnings about a "rank deficient" matrix - you might have too many predictors for the number of observations.

```{r}
fitControl <- trainControl(method = "cv", number = 100)
set.seed(825)

reg.cv <- 
  train(SalePrice ~ ., data = st_drop_geometry(boston.sf) %>% 
                                dplyr::select(SalePrice, 
                                LivingArea, Style, GROSS_AREA, 
                                NUM_FLOORS.cat, R_BDRMS, R_FULL_BTH, 
                                R_HALF_BTH, R_KITCH, R_AC, 
                                R_FPLACE, crimes.Buffer), 
     method = "lm", trControl = fitControl, na.action = na.pass)

reg.cv

```
`reg.cv` is an object that contains a data frame called `resample` with your folds and errors associated with them. You will use this to make a plot of your errors for the CV process.

```{r}
reg.cv$resample[1:5,] %>%
  pander()

```

## Spatial Lags

What is the relationship between errors? Are they clustered? Is the error of a single observation correlated with the error of nearby observations?

We create a list of "neigbhors" using a "spatial weights matrix".

```{r}
coords <- st_coordinates(boston.sf) 

neighborList <- knn2nb(knearneigh(coords, 5))

spatialWeights <- nb2listw(neighborList, style="W")

boston.sf$lagPrice <- lag.listw(spatialWeights, boston.sf$SalePrice)

```



```{r}
coords.test <-  st_coordinates(boston.test) 

neighborList.test <- knn2nb(knearneigh(coords.test, 5))

spatialWeights.test <- nb2listw(neighborList.test, style="W")
 
boston.test %>% 
  mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)) %>%
  ggplot()+
  geom_point(aes(x =lagPriceError, y =SalePrice.Error), color = "orange") +
  labs(title = "Spatial Lag of Sale Price Error",
       x = "Lag of Sale Price Error",
       y = "Sale Price Error") +
  theme_minimal()

```

## Do Errors Cluster? Using Moran's I

So - is your Moran's I statistic indicating dispersion (-1), randomness (0) or clustering (1)?


```{r}
moranTest <- moran.mc(boston.test$SalePrice.Error, 
                      spatialWeights.test, nsim = 999)

ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "orange",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Observed and permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count") +
  theme_minimal()

```

## Predictions by neighborhood

```{r}
boston.test %>%
as.data.frame() %>%
  group_by(Name) %>%
    summarize(meanPrediction = mean(SalePrice.Predict),
              meanPrice = mean(SalePrice)) %>%
      pander(caption = "Mean Predicted and Actual Sale Price by Neighborhood")

```

## Regression with neighborhood effects

Let's try to run the regression again, but this time with a neighborhood fixed effect

```{r}
reg.nhood <- lm(SalePrice ~ ., data = as.data.frame(boston.training) %>% 
                                 dplyr::select(Name, SalePrice, LivingArea, 
                                               Style, GROSS_AREA, NUM_FLOORS.cat,
                                               R_BDRMS, R_FULL_BTH, R_HALF_BTH, 
                                               R_KITCH, R_AC, R_FPLACE,crimes.Buffer))

boston.test.nhood <-
  boston.test %>%
  mutate(Regression = "Neighborhood Effects",
         SalePrice.Predict = predict(reg.nhood, boston.test),
         SalePrice.Error = SalePrice.Predict- SalePrice,
         SalePrice.AbsError = abs(SalePrice.Predict- SalePrice),
         SalePrice.APE = (abs(SalePrice.Predict- SalePrice)) / SalePrice)%>%
  filter(SalePrice < 5000000)

```

How do these models compare? We can bind our error info together and then examine!

```{r}
bothRegressions <- 
  rbind(
    dplyr::select(boston.test, starts_with("SalePrice"), Regression, Name) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)),
    dplyr::select(boston.test.nhood, starts_with("SalePrice"), Regression, Name) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)))  

```

Why do these values differ from those in the book?

```{r}
st_drop_geometry(bothRegressions) %>%
  gather(Variable, Value, -Regression, -Name) %>%
  filter(Variable == "SalePrice.AbsError" | Variable == "SalePrice.APE") %>%
  group_by(Regression, Variable) %>%
    summarize(meanValue = mean(Value, na.rm = T)) %>%
    spread(Variable, meanValue) %>%
    pander(caption = "Mean Absolute Error and Absolute Percentage Error by Regression")
```

## Further examination of errors

Predicted versus observed plots - what does it mean if the line is above or below y=x?

```{r}
bothRegressions %>%
  dplyr::select(SalePrice.Predict, SalePrice, Regression) %>%
    ggplot(aes(SalePrice, SalePrice.Predict)) +
  geom_point() +
  stat_smooth(aes(SalePrice, SalePrice), 
             method = "lm", se = FALSE, size = 1, colour="orange") + 
  stat_smooth(aes(SalePrice.Predict, SalePrice), 
              method = "lm", se = FALSE, size = 1, colour="olivedrab") +
  facet_wrap(~Regression) +
  labs(title="Predicted sale price as a function of observed price",
       subtitle="Orange line represents a perfect prediction; Green line represents prediction") +
  theme_minimal()
```

We can also examine the spatial pattern of errors.

```{r}
st_drop_geometry(bothRegressions) %>%
  group_by(Regression, Name) %>%
  summarize(mean.MAPE = mean(SalePrice.APE, na.rm = T)) %>%
  ungroup() %>% 
  left_join(nhoods, by = c("Name" = "neighborhood")) %>%
    st_sf() %>%
    ggplot() + 
      geom_sf(aes(fill = mean.MAPE), color = "transparent") +
      geom_sf(data = bothRegressions, colour = "gray30", size = .5) +
      facet_wrap(~Regression) +
      scale_fill_gradient(low = palette5[1], high = palette5[5],
                          name = "MAPE") +
      labs(title = "Mean test set MAPE by neighborhood") +
      theme_void()

```

## Race and income context of predictions

What is the race and income context of Boston census tracts, and how does this relate to our model performance?

```{r}
tracts17 <- 
  get_acs(geography = "tract", variables = c("B01001_001E","B01001A_001E","B06011_001"), 
          year = 2017, state=25, county=025, geometry=T, output = "wide") %>%
  st_transform('ESRI:102286')  %>%
  rename(TotalPop = B01001_001E,
         NumberWhites = B01001A_001E,
         Median_Income = B06011_001E) %>%
  mutate(percentWhite = NumberWhites / TotalPop,
         raceContext = ifelse(percentWhite > .5, "Majority White", "Majority Non-White"),
         incomeContext = ifelse(Median_Income > 32322, "High Income", "Low Income"))

grid.arrange(ncol = 2,
  ggplot() + geom_sf(data = na.omit(tracts17), aes(fill = raceContext), color = "transparent") +
    scale_fill_manual(values = c("olivedrab", "orange"), name="Race Context") +
    labs(title = "Race Context") +
    theme_void() + theme(legend.position="bottom"), 
  ggplot() + geom_sf(data = na.omit(tracts17), aes(fill = incomeContext), color = "transparent") +
    scale_fill_manual(values = c("olivedrab", "orange"), name="Income Context") +
    labs(title = "Income Context") +
    theme_void() + theme(legend.position="bottom"))

```

```{r}
st_join(bothRegressions, tracts17) %>% 
  filter(!is.na(incomeContext)) %>%
  group_by(Regression, incomeContext) %>%
  summarize(mean.MAPE = scales::percent(mean(SalePrice.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(incomeContext, mean.MAPE) %>%
  pander(caption = "Test set MAPE by neighborhood income context")
```