---
title: "MUSA 508, Lab 5 - Spatial Machine Learning Pt. 2"
author: "Harris, Fichman, Steif - 2024"
date: 'August, 2024'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot)
library(jtools)     # for regression model plots
library(broom)
library(tufte)
library(rmarkdown)
library(pander)
library(tidycensus)
# functions and data directory
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

palette5 <- c("#25CB10", "#5AB60C", "#8FA108",   "#C48C04", "#FA7800")
```

Last week we learned to create OLS regression models to estimate housing prices by wrangling property records and spatial data to engineer independent variables that predict sales prices.

This week, we are going to learn how to validate our models and understand our model errors. Specifically, we are going to measure overall accuracy, and accuracy across geographic and thematic contexts in our data set. 

This will give us all the skills and code we need to put our midterm assignment together.

## Learning objectives

- Build a training and testing environment to validate a regression model

- Examine Errors to understand if they are non-random, in order to diagnose model quality

- Create and examine validation metrics including MAE and MAPE

- Use Moran's I to see if errors are spatially non-random

- Examine generalizability across geographic contexts

## Data Loading

Bring in a wrangled version of our Boston housing prices data set. You will see that we have some new variables we didn't have last week - they are engineered versions of the existing data (a categorical definition of number of floors, for example), and some of our spatial variables about crime - like knn variables based on nearby assault incidents. Use the `glimpse` command to examine the data. The variable titles are fairly self-explanatory.

*Note - loading `boston.sf` is a bit funky here, because the geojson is saved in WGS84 with coordinates in the ESRI:102286 cref. This makes it hard to handle with st_read, so it's coerced to data frame and back to sf*

```{r read_data, results='hide'}
#https://github.com/urbanSpatial/Public-Policy-Analytics-Landing/tree/master/DATA/Chapter3_4


boston.sf <- st_read(file.path(root.dir,"/Chapter3_4/boston_sf_Ch1_wrangled.geojson")) %>% 
  as.data.frame() %>%
  st_as_sf(crs = 'ESRI:102286')

nhoods <- 
  st_read("https://raw.githubusercontent.com/mafichman/musa_5080_2023/main/Week_4/neighborhoods/bost_nhoods.geojson") %>%
  st_transform('ESRI:102286')

```

## Split Data into Train/Test Set

In order to assess model accuracy and generalizability, we split our data into "training" and "testing" sets using `createDataPartition`. We estimate a regression called `reg.training` using a combination of variables representing housing characteristics, spatial "exposure" to disamenities, and some spatial fixed effects (aka "dummy variables").

Let's walk through the process a step at a time.

- Can you figure out what's going on when we create our error metrics?

Create a train/test setup.

*- What is the size of the split in the data partition?*

*- What do the errors messages mean?  If there is a single record in a category - it gets sorted into the training set. Why??*

```{r}

inTrain <- createDataPartition(
              y = paste(boston.sf$Name, boston.sf$NUM_FLOORS.cat, 
                        boston.sf$Style, boston.sf$R_AC), 
              p = .60, list = FALSE)


boston.training <- boston.sf[inTrain,] 
boston.test <- boston.sf[-inTrain,]  
```
Estimate a regression:

*- Discuss the variables in our model ... *


```{r}
reg.training <- 
  lm(SalePrice ~ ., data = as.data.frame(boston.training) %>% 
                             dplyr::select(SalePrice, LivingArea, Style, 
                                           GROSS_AREA, NUM_FLOORS.cat,
                                           R_BDRMS, R_FULL_BTH, R_HALF_BTH, 
                                           R_KITCH, R_AC, R_FPLACE, crimes.Buffer))
```

*Examine the summary... what do you think?*

```{r}

summary(reg.training)

```

Let's use the `predict` function to generate estimates for the houses in our test set `boston.test` using our model `reg.training` inside a `mutate` function.

By comparing our estimate `SalePrice.Predict` to the known `SalePrice` - we can make various error metrics.

*What are the metrics we are creating here?*

```{r}
boston.test <-
  boston.test %>%
  mutate(Regression = "Baseline Regression",
         SalePrice.Predict = predict(reg.training, boston.test),
         SalePrice.Error = SalePrice.Predict - SalePrice,
         SalePrice.AbsError = abs(SalePrice.Predict - SalePrice),
         SalePrice.APE = (abs(SalePrice.Predict - SalePrice)) / SalePrice) %>%
  filter(SalePrice < 5000000) 

```

*What is the MAE? Let's interpret this statistic.*

```{r}
mean(boston.test$SalePrice.AbsError, na.rm = T)

```

*How about the MAPE?*

```{r}
mean(boston.test$SalePrice.APE, na.rm = T)

```

## Cross-Validation

Our error metrics are a product of the data that were sorted into our training and test sets. If you were to do this analysis using another random subset, your model specifications would be slightly different, and your MAE and MAPE would be different as well.  So... on average, how well do we do using these model parameters? We use random k-fold cross-validation.

To set up an iteration of random 'folds' (in this case 100), we use our very same regression formula to train a different regression object `reg.cv` that is subjected to the cross-validation process for the whole data set.

If you get warnings about a "rank deficient" matrix - you might have too many predictors for the number of observations. This is probably occurring because `Style` has so many levels.

```{r}
fitControl <- trainControl(method = "cv", number = 100)
set.seed(825)

reg.cv <- 
  train(SalePrice ~ ., data = st_drop_geometry(boston.sf) %>% 
                                dplyr::select(SalePrice, 
                                LivingArea, Style, GROSS_AREA, 
                                NUM_FLOORS.cat, R_BDRMS, R_FULL_BTH, 
                                R_HALF_BTH, R_KITCH, R_AC, 
                                R_FPLACE, crimes.Buffer), 
     method = "lm", trControl = fitControl, na.action = na.pass)

reg.cv

```
`reg.cv` is an object that contains a data frame called `resample` with your folds and errors associated with them. You will use this to make a plot of your errors for the CV process.

Here we can take a look at the first few rows of `reg.cv` and we can see how these statistics differ from fold to fold.

```{r}
reg.cv$resample[1:5,] %>%
  pander()

```

## Spatial Lags

What is the relationship between errors? Are they clustered? Is the error of a single observation correlated with the error of nearby observations?

We need to look at the relationships between our observations and their neighbors to see if there is... SPATIAL AUTOCORRELATION.

If we have spatial autocorrelation of our errors, that means that there is some spatial information that our model is not "explaining".

To do this, we first create a list of "neighbors" using a "spatial weights matrix".

We can use this technique to look at the "spatial lag" of nearby prices.

```{r}
coords <- st_coordinates(boston.sf) 

neighborList <- knn2nb(knearneigh(coords, 5))

spatialWeights <- nb2listw(neighborList, style="W")

boston.sf$lagPrice <- lag.listw(spatialWeights, boston.sf$SalePrice)

```


Let's look at our test set - what's the relationship between the prediction error for a given house, and that of the neighborhing house. Do these look correlated when we make a scatter plot?


```{r}
coords.test <-  st_coordinates(boston.test) 

neighborList.test <- knn2nb(knearneigh(coords.test, 5))

spatialWeights.test <- nb2listw(neighborList.test, style="W")
 
boston.test %>% 
  mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)) %>%
  ggplot()+
  geom_point(aes(x =lagPriceError, y =SalePrice.Error), color = "orange") +
  labs(title = "Spatial Lag of Sale Price Error",
       x = "Lag of Sale Price Error",
       y = "Sale Price Error") +
  theme_minimal()

```

We can calculate the Pearson's R coefficient to test this a different way

```{r}

pearsons_test <- 
  boston.test %>% 
  mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error))

cor.test(pearsons_test$lagPriceError,
         pearsons_test$SalePrice.Error, 
         method = "pearson")

```

## Do Errors Cluster? Using Moran's I

We can assess the global degree of clustering or dispersion of sales price values by using the Moran's I statistic, which compares the degree of observed clustering to a simulated distribution of clustering.

Consider the simulation in this way - if you took all the values in the data set, and shook them up and randomly threw them back on the map - what would the level of clustering or dispersion would you observe?

So - is your Moran's I statistic indicating dispersion (-1), randomness (0) or clustering (1)?


```{r}
moranTest <- moran.mc(boston.test$SalePrice.Error, 
                      spatialWeights.test, nsim = 999)

ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "orange",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Observed and permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count") +
  theme_minimal()

```

## Predictions by neighborhood

To test our model's generalizeability, we can summarize the predictions by neighborhood (which is called `Name` in our data set).

*Can you adjust the code below to calculate the MAE and MAPE by neighborhood?*

```{r}
boston.test %>%
as.data.frame() %>%
  group_by(Name) %>%
    summarize(meanPrediction = mean(SalePrice.Predict),
              meanPrice = mean(SalePrice)) %>%
      pander(caption = "Mean Predicted and Actual Sale Price by Neighborhood")

```

## Regression with neighborhood effects

We noticed that we have some clustering in our errors. This means that some elements of the spatial process are unaccounted for - namely, the effect of neighborhood. Up until now, we are basically treating all of the houses as being same except for their structural characteristics, but if you've ever searched for a house or an apartment, you know that it's "location, location, location."

Let's try to run the regression again, but this time with a neighborhood fixed effect...



```{r}
reg.nhood <- lm(SalePrice ~ ., data = as.data.frame(boston.training) %>% 
                                 dplyr::select(Name, SalePrice, LivingArea, 
                                               Style, GROSS_AREA, NUM_FLOORS.cat,
                                               R_BDRMS, R_FULL_BTH, R_HALF_BTH, 
                                               R_KITCH, R_AC, R_FPLACE,crimes.Buffer))
```

Take a look at the summary, how does it compare to the last model?

```{r}
summary(reg.nhood)

```

Let's calculate our top-level error metrics for this new model.

```{r}
boston.test.nhood <-
  boston.test %>%
  mutate(Regression = "Neighborhood Effects",
         SalePrice.Predict = predict(reg.nhood, boston.test),
         SalePrice.Error = SalePrice.Predict- SalePrice,
         SalePrice.AbsError = abs(SalePrice.Predict- SalePrice),
         SalePrice.APE = (abs(SalePrice.Predict- SalePrice)) / SalePrice)%>%
  filter(SalePrice < 5000000)

```

How do these models compare? We can bind our error info together and then create a table to examine the MAE AND MAPE for each model.

```{r}
bothRegressions <- 
  rbind(
    dplyr::select(boston.test, starts_with("SalePrice"), Regression, Name) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)),
    dplyr::select(boston.test.nhood, starts_with("SalePrice"), Regression, Name) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)))  

```


Why do these values differ from those in the book?

```{r}
st_drop_geometry(bothRegressions) %>%
  gather(Variable, Value, -Regression, -Name) %>%
  filter(Variable == "SalePrice.AbsError" | Variable == "SalePrice.APE") %>%
  group_by(Regression, Variable) %>%
    summarize(meanValue = mean(Value, na.rm = T)) %>%
    spread(Variable, meanValue) %>%
    pander(caption = "Mean Absolute Error and Absolute Percentage Error by Regression")
```

## Further examination of errors

Predicted versus observed plots are extremely useful for seeing what kinds of observations our model is predicting well, and which kinds it's not.

*What does it mean if the line is above or below y=x?*

*What are your observations about the differences in model performance?*

```{r}
bothRegressions %>%
  dplyr::select(SalePrice.Predict, SalePrice, Regression) %>%
    ggplot(aes(SalePrice, SalePrice.Predict)) +
  geom_point() +
  stat_smooth(aes(SalePrice, SalePrice), 
             method = "lm", se = FALSE, size = 1, colour="orange") + 
  stat_smooth(aes(SalePrice.Predict, SalePrice), 
              method = "lm", se = FALSE, size = 1, colour="olivedrab") +
  facet_wrap(~Regression) +
  labs(title="Predicted sale price as a function of observed price",
       subtitle="Orange line represents a perfect prediction; Green line represents prediction") +
  theme_minimal()
```

### Spatial evaluation of errors

We can also examine the spatial pattern of errors, looking to see if we are performing equally well from neighborhood to neighborhood. Since we have our dataset `bothRegressions` - which has observed prices and predictions from each model, we can summarize by model, and by neighborhood, and map these predictions.


```{r}
st_drop_geometry(bothRegressions) %>%
  group_by(Regression, Name) %>%
  summarize(mean.MAPE = mean(SalePrice.APE, na.rm = T)) %>%
  ungroup() %>% 
  left_join(nhoods, by = c("Name" = "neighborhood")) %>%
    st_sf() %>%
    ggplot() + 
      geom_sf(aes(fill = 100*mean.MAPE), color = "transparent") +
      geom_sf(data = bothRegressions, colour = "gray30", size = .5) +
      facet_wrap(~Regression) +
      scale_fill_gradient(low = palette5[1], high = palette5[5],
                          name = "MAPE") +
      labs(title = "Mean test set MAPE by neighborhood") +
      theme_void()

```

## Race and income context of predictions

What is the race and income context of Boston census tracts, and how does this relate to our model performance?

We can call some ACS data from `tidycensus` and segment the city into different tract typologies - majority white, high income, low income, and so on.

The income cutoff here refers (I believe) to the mean of the tract Median_Income across Boston.

```{r}
tracts17 <- 
  get_acs(geography = "tract", 
          variables = c("B01001_001E","B01001A_001E","B06011_001"), 
          year = 2017, 
          state=25, 
          county=025, 
          geometry=T, 
          output = "wide") %>%
  st_transform('ESRI:102286')  %>%
  rename(TotalPop = B01001_001E,
         NumberWhites = B01001A_001E,
         Median_Income = B06011_001E) %>%
  mutate(percentWhite = NumberWhites / TotalPop,
         raceContext = ifelse(percentWhite > .5, "Majority White", "Majority Non-White"),
         incomeContext = ifelse(Median_Income > 35322, "High Income", "Low Income"))
```


Let's create a context map so we can see what areas these are.


```{r}
grid.arrange(ncol = 2,
  ggplot() + geom_sf(data = na.omit(tracts17), aes(fill = raceContext), color = "transparent") +
    scale_fill_manual(values = c("olivedrab", "orange"), name="Race Context") +
    labs(title = "Race Context") +
    theme_void() + theme(legend.position="bottom"), 
  ggplot() + geom_sf(data = na.omit(tracts17), aes(fill = incomeContext), color = "transparent") +
    scale_fill_manual(values = c("olivedrab", "orange"), name="Income Context") +
    labs(title = "Income Context") +
    theme_void() + theme(legend.position="bottom"))

```

We can now summarize our predictions by tract type - joining our sf objects `bothRegressions` and `tracts17` and calculating the mean MAPE for each regression type and each tract typology.

*How do you think our models are performing?*

```{r}
st_join(bothRegressions, tracts17) %>% 
  filter(!is.na(incomeContext)) %>%
  group_by(Regression, incomeContext) %>%
  summarize(mean.MAPE = 100*(mean(SalePrice.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(incomeContext, mean.MAPE) %>%
  pander(caption = "Test set MAPE by neighborhood income context")
```