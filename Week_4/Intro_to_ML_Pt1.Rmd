---
title: "MUSA 508, Lab 4 - Spatial Machine Learning Pt. 1"
author: "Harris, Fichman, and Steif - 2024"
output: html_document
---

```{r setup, include=FALSE}

# You can set some global options for knitting chunks

knitr::opts_chunk$set(echo = TRUE)

# Load some libraries

library(tidyverse)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot) # plot correlation plot
library(corrr)      # another way to plot correlation plot
library(kableExtra)
library(jtools)     # for regression model plots
library(ggstance) # to support jtools plots
library(ggpubr)    # plotting R^2 value on ggplot point scatter
library(broom.mixed) # needed for effects plots

# functions and data directory
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

palette5 <- c("#25CB10", "#5AB60C", "#8FA108",   "#C48C04", "#FA7800")
```

# MUSA 508, Lab 4 - Spatial Machine Learning Pt. 1

Our research question here is as follows - how well can we predict Boston housing prices using an ordinary least squares regression model that estimates housing price as a function of structural features and nearby amenities/disamenities like crime?

The learning objectives of this lab are:

*   Review Topics: loading data, {dplyr}, mapping and plotting with {ggplot2}

*   Understanding spatial indicators with K-Nearest Neighbor and the `nn_function()` function.

*   Execute a data-wrangling and modeling workflow - from data intake to wrangling to engineering to modeling

*   Understanding Simple correlation and the Pearson's r - Correlation Coefficient

*   Linear Regression models - goodness-of-fit and R2 - Coefficient of Determination

The in-class exercise at the end of this lab gives you an opportunity to modify the existing code, create new features, and subsequently estimate and interpret new regression models. 

## Data Wrangling

Let's start by reading in data describing Boston neighborhoods (`nhoods`), housing sales (`boston`) and crime incident reports (`boston Crimes`).

*See if you can adjust this code block to suppress text outputs that show up in your markdown*

```{r read_data}

nhoods <- 
  st_read("https://raw.githubusercontent.com/mafichman/musa_5080_2024/main/Week_4/neighborhoods/bost_nhoods.geojson") %>%
  st_transform('ESRI:102286')

boston <- 
  read.csv(file.path(root.dir,"/Chapter3_4/bostonHousePriceData_clean.csv"))

bostonCrimes <- read.csv(file.path(root.dir,"/Chapter3_4/bostonCrimes.csv"))

```

Our housing prices are a csv file - let's turn them into an sf object.

```{r}
boston.sf <- 
  boston %>% 
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102286')
```

### Exploratory data analysis


Let's examine our data - summarize the number of crimes of each type.

```{r EDA}

bostonCrimes %>% 
group_by(OFFENSE_CODE_GROUP) %>%
  summarize(count = n()) %>%
  arrange(-count) %>% 
  top_n(10) %>%
  kable() %>%
  kable_styling()
```


*Can you make a histogram describing the distribution of housing prices in our `boston` data set?*

### Mapping 

See if you can alter the size of the image output. Search for adjusting figure height and width for a rmarkdown block

```{r price_map}
# ggplot, reorder

# Mapping data
ggplot() +
  geom_sf(data = nhoods, fill = "grey40") +
  geom_sf(data = boston.sf, aes(colour = q5(PricePerSq)), 
          show.legend = "point", size = .75) +
  scale_colour_manual(values = palette5,
                   labels=qBr(boston,"PricePerSq"),
                   name="Quintile\nBreaks") +
  labs(title="Price Per Square Foot, Boston") +
  theme_void()
```

### Cleaning Crime Data

Let's filter for only a particular type of offense, Aggravated Assault, and spatialize those data using the latitude and longitude.

We turn it from data frame into sf using `st_as_sf` and then keep only distinct observations.

Why are we using a filter for `Lat > -1` and doing an `na.omit`? Because there are some bad data in here? This happens all the time - so be aware of poorly coded or missing data in your project!

```{r clean_crime}
bostonCrimes.sf <-
  bostonCrimes %>%
    filter(OFFENSE_CODE_GROUP == "Aggravated Assault",
           Lat > -1) %>%
    dplyr::select(Lat, Long) %>%
    na.omit() %>%
    st_as_sf(coords = c("Long", "Lat"), crs = "EPSG:4326") %>%
    st_transform('ESRI:102286') %>%
    distinct()

```

### Create Nearest Spatial Features

This is where we do two important things:

*   aggregate values within a buffer, and 
*   create  `nearest neighbor` features. 

These are primary tools we use to add the local spatial signal to each of the points/rows/observations we are modeling. 

#### Buffer aggregate

The first code in this block buffers each point in `boston.sf` by `660` ft and then uses `aggregate` to count the number of `bostonCrimes.sf` points within that buffer. There is a nested call to `mutate` to assign a value of `1` for each `bostonCrimes.sf` as under a column called `counter`. This allows each crime to have the same weight in when the `sun` function is called, but other weighting schemes could be used. Finally, the code `pull` pulls the aggregate values so they can be assigned as the `crimes.Buffer` column to `boston.sf`. This is a little different from how you had assigned new columns before (usually using `mutate`), but valid. Your new feature `crimes.Buffer` is a count of all the crimes within 660ft of each reported crime. Why would it be god to know this when building a model?  

```{r Features}

# Counts of crime per buffer of house sale
boston.sf$crimes.Buffer <- boston.sf %>% 
    st_buffer(660) %>% 
    aggregate(mutate(bostonCrimes.sf, counter = 1),., sum) %>%
    pull(counter)

```


#### k nearest neighbor (knn)

The second block of code is using knn for averaging or summing values of a set number of (referred to as `k`) the nearest observations to each observation; it's *nearest neighbors*. With this, the model can understand the magnitude of values that are *near* each point. This adds a spatial signal to the model. 

<!-- The function `nn_function()` is a custom function from our textbook. It takes two pairs of **coordinates** form two `sf` **point** objects.

Using `st_coordinates()` within `mutate` converts the points to longitude/latitude coordinate pairs for the `nn_function()` to work with. 

If you put *polygon* features in there, it will error. Instead you can use `st_coordinates(st_centroid(YourPolygonSfObject))` to get nearest neighbors to a polygon centroid. -->

The number `k` in the `nn_function()` function is the number of neighbors to to values from that are then averaged. Different types of crime (or anything else you measure) will require different values of `k`. *You will have to think about this!*. What could be the importance of `k` when you are making knn features for a violent crime versus and nuisance crime?


```{r knn}
boston.sf <-
  boston.sf %>% 
    mutate(
      crime_nn1 = nn_function(st_coordinates(boston.sf), 
                              st_coordinates(bostonCrimes.sf), k = 1),
      
      crime_nn2 = nn_function(st_coordinates(boston.sf), 
                              st_coordinates(bostonCrimes.sf), k = 2), 
      
      crime_nn3 = nn_function(st_coordinates(boston.sf), 
                              st_coordinates(bostonCrimes.sf), k = 3), 
      
      crime_nn4 = nn_function(st_coordinates(boston.sf), 
                              st_coordinates(bostonCrimes.sf), k = 4), 
      
      crime_nn5 = nn_function(st_coordinates(boston.sf), 
                              st_coordinates(bostonCrimes.sf), k = 5)) 
```

```{r assault density}
## Plot assault density
ggplot() + geom_sf(data = nhoods, fill = "grey40") +
  stat_density2d(data = data.frame(st_coordinates(bostonCrimes.sf)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_gradient(low = "#25CB10", high = "#FA7800", name = "Density") +
  scale_alpha(range = c(0.00, 0.35), guide = "none") +
  labs(title = "Density of Aggravated Assaults, Boston") +
  theme_void()
```

## Analyzing associations

One of the key steps in the data wrangling process is to determine which variables are correlated with your dependent variable or outcome (housing prices), and also which variables are correlated with each other. To do this, we use scatterplots to examine variable combinations.

### Pearson's r - Correlation Coefficient

Pearson's r Learning links:
*   [Pearson Correlation Coefficient (r) | Guide & Examples](https://www.scribbr.com/statistics/pearson-correlation-coefficient/)
*   [Correlation Test Between Two Variables in R](http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r)

Note: the use of the `ggscatter()` function from the `ggpubr` package to plot the *Pearson's rho* or *Pearson's r* statistic; the Correlation Coefficient. This number can also be squared and represented as `r2`. However, this differs from the `R^2` or `R2` or "R-squared" of a linear model fit, known as the Coefficient of Determination. This is explained a bit more below.

```{r uni_variate_Regression}
boston_sub_200k <- st_drop_geometry(boston.sf) %>% 
filter(SalePrice <= 2000000) 

cor.test(boston_sub_200k$LivingArea,
         boston_sub_200k$SalePrice, 
         method = "pearson")

ggscatter(boston_sub_200k,
          x = "LivingArea",
          y = "SalePrice",
          add = "reg.line") +
  stat_cor(label.y = 2500000) 

```


The Pearson's rho - Correlation Coefficient and the R2 Coefficient of Determination are **very** frequently confused! It is a really common mistake, so take a moment to understand what they are and how they differ. [This blog](https://towardsdatascience.com/r%C2%B2-or-r%C2%B2-when-to-use-what-4968eee68ed3) is a good explanation. In summary:

*   The `r` is a measure the degree of relationship between two variables say x and y. It can go between -1 and 1.  1 indicates that the two variables are moving in unison.

*   However, `R2` shows percentage variation in y which is explained by all the x variables together. Higher the better. It is always between 0 and 1. It can never be negative – since it is a squared value.



### Exploratory analysis of correlations

We are going to want to look for correlation all over our data set to see what might be good in a model. We can use a wide-to-long transformation to do this for several *continuous* variables at once.

*Run these code blocks...*
Notice the use of `st_drop_geometry()`, this is the correct way to go from a `sf` spatial dataframe to a regular dataframe with no spatial component.

*Can somebody walk me through what they do?*

*Can you give me a one-sentence description of what the takeaway is?*


```{r Correlation}

## Home Features cor
boston.sf %>%
  st_drop_geometry() %>%
  mutate(Age = 2015 - YR_BUILT) %>%
  dplyr::select(SalePrice, LivingArea, Age, GROSS_AREA) %>%
  filter(SalePrice <= 1000000, Age < 500) %>%
  gather(Variable, Value, -SalePrice) %>% 
   ggplot(aes(Value, SalePrice)) +
     geom_point(size = .5) + 
  geom_smooth(method = "lm", se=F, colour = "#FA7800") +
     facet_wrap(~Variable, ncol = 3, scales = "free") +
     labs(title = "Price as a function of continuous variables") +
     theme_minimal()
```

```{r crime_corr}
## Crime cor
boston.sf %>%
  st_drop_geometry() %>%
  mutate(Age = 2015 - YR_BUILT) %>%
  dplyr::select(SalePrice, starts_with("crime_")) %>%
  filter(SalePrice <= 1000000) %>%
  gather(Variable, Value, -SalePrice) %>% 
   ggplot(aes(Value, SalePrice)) +
     geom_point(size = .5) + geom_smooth(method = "lm", se=F, colour = "#FA7800") +
     facet_wrap(~Variable, nrow = 1, scales = "free") +
     labs(title = "Price as a function of continuous variables") +
     theme_minimal()
```

## Correlation matrix

An alternative approach - we can use a correlation matrix gives us the pairwise correlation of each set of features in our data. It is usually advisable to include the target/outcome variable in this so we can understand which features are related to it. We feed all of our continuous variables to the `ggcorrpolot` function.

Some things to notice in this code; we use `select_if()` to select only the features that are numeric. This is really handy when you don't want to type or hard-code the names of your features; `ggcorrplot()` is a function from the `ggcorrplot` package.

**Let's take a few minutes to interpret this**

```{r correlation_matrix}
numericVars <- 
  select_if(st_drop_geometry(boston.sf), is.numeric) %>% na.omit()

ggcorrplot(
  round(cor(numericVars), 1), 
  p.mat = cor_pmat(numericVars),
  colors = c("#25CB10", "white", "#FA7800"),
  type="lower",
  insig = "blank") +  
    labs(title = "Correlation across numeric variables") 

# yet another way to plot the correlation plot using the corrr library
numericVars %>% 
  correlate() %>% 
  autoplot() +
  geom_text(aes(label = round(r,digits=2)),size = 2)

```

## Univarite Regression

### R2 - Coefficient of Determination

Discussed above, the `R^2` or "R-squared" is a common way to validate the predictions of a linear model. Below we run a linear model on our data with the `lm()` function and get the output in our R terminal. At first this is an intimidating amount of information! Here is a [great resource](https://towardsdatascience.com/understanding-linear-regression-output-in-r-7a9cbda948b3) to understand how that output is organized and what it means.  

What we are focusing on here is that `R-squared`,  `Adjusted R-squared` and the `Coefficients`.

Let's make a simple model - `SalePrice` as a function of `LivingArea` using the `lm` function to make a "linear model."  The `summary` function let's us look at the model parameters and statistics.

*What's the `R2` good for as a diagnostic of model quality?*

*Can somebody interpret the coefficient?*

```{r simple_reg}
livingReg <- lm(SalePrice ~ LivingArea, data = boston_sub_200k)

summary(livingReg)

```

## Prediction example

Our regression not only estimates relationships between the DV and the independent variables, but it also can be used to predict the DV given some combination of independent variables.

Let's say we had an unobserved house with a `LivingArea` of 400, we could use our regression `livingReg` and the coefficients to estimate it's value -

estimate = error + coefficient * LivingArea

```{r calculate prediction}
coefficients(livingReg)

new_LivingArea = 4000

# "by hand"
378370.01571  + 88.34939 * new_LivingArea

```


The `predict` function will be what we use to do this going forward.

```{r}
predict(livingReg, newdata = data.frame(LivingArea = 4000))
```


## Multivariate Regression

We are going to make more sophisticated models with numerous variables - mulitvariate regression models - that include structural features and spatial features.

Let's create a multivariate regression - notice how we are feeding our data to the `lm` function by sending the whole data set and using `select` inside the lm call.

*What's up with these categorical variables?*

*Better R-squared - does that mean it's a better model?*

```{r mutlivariate_regression}
reg1 <- lm(SalePrice ~ ., data = boston_sub_200k %>% 
                                 dplyr::select(SalePrice, LivingArea, Style, 
                                               GROSS_AREA, R_TOTAL_RM, NUM_FLOORS,
                                               R_BDRMS, R_FULL_BTH, R_HALF_BTH, 
                                               R_KITCH, R_AC, R_FPLACE))

summary(reg1)
```

## Challenges:

-Build a new regression (`reg2`) with LivingArea and crime_nn2? Report the regression coefficient for LivingArea. Is it different than it was before? Why?

- Try to engineer a 'fixed effect' out of the other variables in an attempt to parameterize a variable that suggests a big or fancy house or levels of fanciness. How does this affect your model?

```{r}


```



## Appendix - Marginal Response Plots

Let's try some of these out. They help you learn more about the relationships in the model.

What does a long line on either side of the blue circle suggest?

What does the location of the blue circle relative to the center line at zero suggest?

```{r effect_plots, eval = FALSE}
## Plot of marginal response
effect_plot(reg1, pred = R_BDRMS, interval = TRUE, plot.points = TRUE)

## Plot coefficients
plot_summs(reg1, scale = TRUE)

## plot multiple model coeffs
plot_summs(reg1, livingReg)


```